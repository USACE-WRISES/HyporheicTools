# functions/model_utils.py
"""
Reusable helpers that are NOT tied to a single model run.

Why lift them out?
------------------
* keeps ``__main__.py`` focused on the modelling workflow;
* makes unit-testing trivial;
* avoids “copy-paste-drift” when you use the same logic in another project.

Contents
~~~~~~~~
1.  Boundary helpers
    • boundary_start_end()       – pick first/last vertex of a line layer
    • make_up_down_stream()      – build upstream / downstream center-lines

2.  Grid–domain helpers
    • build_grid_polygons()      – rectangular polygons for every grid cell
    • idomain_from_domain()      – 3-D active/ inactive mask (idomain array)

All helpers are *side-effect-free*: they return data instead of mutating
global vars.
"""
from __future__ import annotations

from pathlib import Path
from typing import Tuple, Literal
import math
import matplotlib.pyplot as plt
import flopy
from flopy.plot import PlotMapView
from pathlib import Path, PurePath
from packaging import version                # only needed for the fallback

# import pyvista as pv

import geopandas as gpd
import numpy as np
from shapely.geometry import LineString, Polygon

from shapely.geometry import Point
import pandas as pd
import rasterio
from rasterio.transform import rowcol

import matplotlib.pyplot as plt
from matplotlib.widgets import Slider
from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from pathlib import Path
import numpy as np


def _union_all(geo):
    """Return a single geometry union – Shapely 2.x / 1.x compatible."""
    geom = geo.geometry
    return geom.union_all() if hasattr(geom, "union_all") else geom.unary_union

# ────────────────────────────────────────────────────────────────────
# 1. Boundary helpers
# ────────────────────────────────────────────────────────────────────
def boundary_start_end(
    gdf: gpd.GeoDataFrame,
) -> Tuple[Tuple[float, float], Tuple[float, float]]:
    """
    Return the **first** and **last** coordinate pair of *gdf*’s first geometry.

    Equivalent to the verbose notebook lines:

    ```python
    left_start = left_boundary.geometry.iloc[0].coords[0]   # first vertex
    left_end   = left_boundary.geometry.iloc[-1].coords[-1] # last vertex
    ```
    """
    geom = gdf.geometry.iloc[0]          # assume single-feature line layer
    return geom.coords[0], geom.coords[-1]


def make_up_down_stream(
    left: gpd.GeoDataFrame,
    right: gpd.GeoDataFrame,
    crs,
) -> Tuple[gpd.GeoDataFrame, gpd.GeoDataFrame]:
    """
    Build *centre-lines* that span from left→right boundaries.

    Upstream = line between **left.start** and **right.start**  
    Downstream = line between **left.end** and **right.end**

    Matches the notebook sequence that extracted four points then made two
    `LineString`s and two `GeoDataFrame`s.
    """
    l_start, l_end = boundary_start_end(left)
    r_start, r_end = boundary_start_end(right)

    upstream    = LineString([l_start, r_start])
    downstream  = LineString([l_end,  r_end])

    return (
        gpd.GeoDataFrame(geometry=[upstream], crs=crs),
        gpd.GeoDataFrame(geometry=[downstream], crs=crs),
    )

# ────────────────────────────────────────────────────────────────────
# 2. Grid–domain helpers
# ────────────────────────────────────────────────────────────────────
def build_grid_polygons(
    grid_x: np.ndarray,
    grid_y: np.ndarray,
    dx: float, dy: float,
    crs,
) -> gpd.GeoDataFrame:
    """
    Return a `GeoDataFrame` of rectangular cell polygons.

    *grid_x* / *grid_y* are **cell-centre** coordinates generated by
    `ru.generate_grid_centres()`.

    Equivalent to the old notebook loop that appended  (x±dx/2, y±dy/2)  corners
    into ``grid_cells`` then cast to ``GeoDataFrame``.
    """
    halfx, halfy = dx / 2, dy / 2
    polys = [
        Polygon(
            [
                (x - halfx, y - halfy),
                (x - halfx, y + halfy),
                (x + halfx, y + halfy),
                (x + halfx, y - halfy),
            ]
        )
        for x, y in zip(grid_x.ravel(), grid_y.ravel())
    ]
    return gpd.GeoDataFrame(geometry=polys, crs=crs)


def idomain_from_domain(
    grid_gdf: gpd.GeoDataFrame,
    domain_gdf: gpd.GeoDataFrame,
    nlay: int, nrow: int, ncol: int,
) -> np.ndarray:
    """
    Build the 3-D *idomain* array (1 = active, 0 = inactive).

    Steps (matches notebook):

    1.  Spatial predicate – mark each grid cell polygon that *intersects* the
        groundwater-domain layer.
    2.  Initialise a zero-filled (nlay, nrow, ncol) array.
    3.  For every intersecting polygon, set **all layers** at that (row, col)
        to 1.

    Returns
    -------
    ndarray[int]  shape = (nlay, nrow, ncol)
        Boolean mask ready for MODFLOW 6 ``idomain``.
    """
    domain_union = _union_all(domain_gdf)                 
    # vectorised intersects: much faster than per-cell loop
    grid_gdf["inside"] = grid_gdf.geometry.intersects(domain_union)

    idom = np.zeros((nlay, nrow, ncol), dtype=int)
    for flat_idx, inside in enumerate(grid_gdf["inside"]):
        r, c = divmod(flat_idx, ncol)   # convert flat index → (row, col)
        if inside:
            idom[:, r, c] = 1           # set *all* layers active
    return idom
# ────────────────────────────────────────────────────────────────────
# 3.  Boundary-cell helpers 
# ────────────────────────────────────────────────────────────────────

def identify_boundary_cells(idomain: np.ndarray) -> list[tuple[int, int, int]]:
    """
    Return every active cell (k,i,j) that touches an *inactive* neighbour in
    layer-0.  Exactly the same logic as the nested loops in the notebook.
    """
    nlay, nrow, ncol = idomain.shape
    bcells: set[tuple[int, int, int]] = set()

    for r in range(nrow):
        for c in range(ncol):
            if idomain[0, r, c] == 1:
                if (
                    (r > 0         and idomain[0, r-1, c] == 0) or
                    (r < nrow - 1  and idomain[0, r+1, c] == 0) or
                    (c > 0         and idomain[0, r, c-1] == 0) or
                    (c < ncol - 1  and idomain[0, r, c+1] == 0)
                ):
                    for k in range(nlay):
                        bcells.add((k, r, c))
    return sorted(bcells)


def classify_boundary_cells(
    bcells: list[tuple[int, int, int]],
    grid_gdf: gpd.GeoDataFrame,
    boundaries: dict[str, gpd.GeoDataFrame],
    ncol: int,
) -> dict[str, list[tuple[int, int, int]]]:
    """
    Allocate every (k,i,j) in *bcells* to the closest poly-line in *boundaries*.

    Parameters
    ----------
    bcells : list[(k,i,j)]
        Output from `identify_boundary_cells`.
    boundaries : dict
        Keys ``"left" | "right" | "up" | "down"``  →  line GeoDataFrames.
    ncol : int
        Needed to map (row,col) ⇄ flat index in *grid_gdf*.

    Returns
    -------
    dict[str, list[(k,i,j)]]
        Four lists keyed by the boundary name.
    """
    out = {name: [] for name in boundaries}
    classified = set()

    for k, r, c in bcells:
        if (k, r, c) in classified:
            continue
        cell_poly = grid_gdf.iloc[r * ncol + c].geometry
        # distance to every boundary
        closest = min(boundaries,
                      key=lambda key: cell_poly.distance(boundaries[key].unary_union))
        out[closest].append((k, r, c))
        classified.add((k, r, c))
    return out


# ---------------------------------------------------------------------
# Surface-water raster  →  elevation table at model grid points
# ---------------------------------------------------------------------
def csv_points_elevation(points_gdf: gpd.GeoDataFrame,
                         raster_path: str | Path,
                         out_csv: str | Path | None = None,
                         nodata_values: tuple[int | float, ...] = (-9999,)) -> pd.DataFrame:
    """
    Sample *raster_path* at each point in *points_gdf* and return a clean
    ``DataFrame`` with x, y, elevation.

    * Rows whose elevation is NaN **or** matches any value in
      *nodata_values* **or** the raster’s own ``nodata`` flag are removed.
    * Writing to *out_csv* is optional – pass None to skip.

    """
    import rasterio
    from rasterio.transform import rowcol

    raster_path = Path(raster_path)
    vals: list[float] = []

    with rasterio.open(raster_path) as src:
        band  = src.read(1)
        nodat = src.nodata
        # merge header nodata with user list
        nodata_set = {nodat} if nodat is not None else set()
        nodata_set.update(nodata_values)

        for pt in points_gdf.geometry:
            r, c = rowcol(src.transform, pt.x, pt.y)
            if 0 <= r < src.height and 0 <= c < src.width:
                vals.append(band[r, c])
            else:
                vals.append(np.nan)

    df = points_gdf.copy()
    df["x"] = df.geometry.x
    df["y"] = df.geometry.y
    df["elevation"] = vals

    # 1) drop NaNs
    df = df.dropna(subset=["elevation"])

    # 2) drop numeric sentinels using np.isclose for float safety
    drop_mask = np.zeros(len(df), dtype=bool)
    for nd in nodata_set:
        drop_mask |= np.isclose(df["elevation"], nd)

    df = df.loc[~drop_mask, ["x", "y", "elevation"]]

    # 3) optional CSV
    if out_csv is not None:
        out_csv = Path(out_csv).with_suffix(".csv")
        df.to_csv(out_csv, index=False)
        print(f"Elevation CSV written → {out_csv}")

    return df






# ────────────────────────────────────────────────────────────────────
#  Surface‑water points → grid indices  (origin‑aware)
# ────────────────────────────────────────────────────────────────────
def fit_csv_to_grid(
    df: pd.DataFrame,
    ncol: int,
    nrow: int,
    xmin: float,
    ymin: float,
    xmax: float,
    ymax: float,
    *,
    origin: Literal["upper", "lower"] = "upper",
) -> pd.DataFrame:
    """
    Map real‑world x, y in *df* → integer (col, row) indices.

    Parameters
    ----------
    origin : {"upper", "lower"}, default "upper"
        Must match the flag used in ``generate_grid_centres()``.
    """
    dx = (xmax - xmin) / ncol
    dy = (ymax - ymin) / nrow

    out = df.copy()
    out["x_idx"] = ((out["x"] - xmin) / dx).astype(int)

    if origin == "upper":
        # row 0 is the *top* of the model → count from ymax downward
        out["y_idx"] = ((ymax - out["y"]) / dy).astype(int)
    else:                      # "lower" (legacy)
        out["y_idx"] = ((out["y"] - ymin) / dy).astype(int)

    return out


# ────────────────────────────────────────────────────────────────────
#  River‑stage cells  (single authoritative version)
# ────────────────────────────────────────────────────────────────────
def extract_river_cells(
    df: pd.DataFrame,          # expects x_idx / y_idx already present
    idomain: np.ndarray,       # (nlay, nrow, ncol)
    tops:  list[np.ndarray],   # per‑layer top arrays
    botms: list[np.ndarray],   # per‑layer botm arrays
) -> list[tuple[int, int, int, float]]:
    """
    Return ``[(k, i, j, stage), …]`` for every CSV point that lands on an
    *active* column of the model grid.

    • Chooses the correct *k* with ``layer_for_elevation()``  
    • Skips points outside the grid or on inactive columns.  
    • Points above the surface clamp to layer 0.
    """
    nlay, nrow, ncol = idomain.shape
    out: list[tuple[int, int, int, float]] = []

    for _, row in df.iterrows():
        j = int(row.x_idx)          # column
        i = int(row.y_idx)          # row   (already flipped if origin="upper")
        h = float(row.elevation)    # stage

        if not (0 <= i < nrow and 0 <= j < ncol):
            continue
        if not np.any(idomain[:, i, j]):     # whole column inactive
            continue

        k = layer_for_elevation(tops, botms, i, j, h)
        if k is not None:
            out.append((k, i, j, h))

    return out


# ────────────────────────────────────────────────────────────────────
# 5.  Groundwater head helpers  
# ────────────────────────────────────────────────────────────────────
def interpolate_gw_elevation(
    first_layer_cells: list[tuple[int, int, int]],
    head_first: float,
    head_last: float,
    nlay: int,
) -> list[float]:
    """
    Linearly interpolate from *head_first* → *head_last* across the first-layer
    boundary cells, then replicate that 1-D profile down all *nlay* layers.
    """
    n = len(first_layer_cells)
    if n <= 1:
        profile = [head_first] * n
    else:
        profile = [
            head_first + (head_last - head_first) * idx / (n - 1)
            for idx in range(n)
        ]
    return profile * nlay


def build_chd_data(
    river_cells: list[tuple[int, int, int, float]],
    boundary_groups: dict[str, list[tuple[int, int, int]]],
    heads: dict[str, list[float]],
) -> list[list]:
    """
    Assemble `[k, i, j, h]` rows for the MODFLOW-6 CHD package, avoiding
    duplicates (river stage has priority).
    """
    unique: set[tuple[int, int, int]] = set()
    chd:   list[list] = []

    # 1) river cells first (they override anything else)
    for k, i, j, stage in river_cells:
        if (k, i, j) not in unique:
            chd.append([k, i, j, float(stage)])
            unique.add((k, i, j))

    # 2) each lateral boundary
    for key, cells in boundary_groups.items():
        head_vec = heads[key]
        for idx, (k, i, j) in enumerate(cells):
            if (k, i, j) not in unique:
                chd.append([k, i, j, float(head_vec[idx])])
                unique.add((k, i, j))

    return chd

# ──────────────────────────────────────────────────────────────────────
# Boundary-cell helpers
# ──────────────────────────────────────────────────────────────────────
def identify_boundary_cells(idomain: np.ndarray) -> list[tuple[int, int, int]]:
    """
    Return a list of (k, i, j) cells that are **active** in layer-0 and have at
    least one inactive neighbour (left/right/up/down).

    Parameters
    ----------
    idomain : bool/int ndarray (nlay, nrow, ncol)

    Notes
    -----
    • Only the geometry of layer-0 is inspected – the same (i, j) location is
      returned for every layer so the caller can decide what to do.
    """
    kmax, nrow, ncol = idomain.shape
    out: list[tuple[int, int, int]] = []

    for i in range(nrow):
        for j in range(ncol):
            if idomain[0, i, j] == 0:
                continue
            if (
                (i > 0         and idomain[0, i - 1, j] == 0) or
                (i < nrow - 1  and idomain[0, i + 1, j] == 0) or
                (j > 0         and idomain[0, i, j - 1] == 0) or
                (j < ncol - 1  and idomain[0, i, j + 1] == 0)
            ):
                for k in range(kmax):
                    out.append((k, i, j))
    return out


# def classify_boundary_cells(
#     boundary_cells      : list[tuple[int, int, int]],
#     grid_polygons       : gpd.GeoDataFrame,
#     lines               : dict[str, gpd.GeoDataFrame],
#     ncol                : int,
# ) -> dict[str, list[tuple[int, int, int]]]:
#     """
#     Split *boundary_cells* into groups keyed by ``lines`` (left/right/…).

#     Returns
#     -------
#     dict  {name: list[(k,i,j), …]}
#     """
#     groups: dict[str, list] = {k: [] for k in lines}
#     for k, i, j in boundary_cells:
#         cell_poly = grid_polygons.iloc[i * ncol + j].geometry
#         closest = min(lines,
#                       key=lambda name: cell_poly.distance(lines[name].unary_union))
#         groups[closest].append((k, i, j))
#     return groups
# ---------------------------------------------------------------------------
# Boundary-cell classification (vectorised – Shapely 2+, PyGEOS backend)
# ---------------------------------------------------------------------------
def classify_boundary_cells_fast(
        boundary_cells: list[tuple[int,int,int]],
        grid_polys: gpd.GeoDataFrame,
        lines: dict[str, gpd.GeoDataFrame],
        ncol: int,
) -> dict[str, list[tuple[int,int,int]]]:
    """
    Return dict {left|right|up|down: [cells]} using *vectorised* distance calls.
    Works best with Shapely >= 2.0 (PyGEOS backend).
    """
    # -- 1. one geometry per bank (union + prepare)
    bank_geom = {name: lines[name].geometry.unary_union    # Shapely < 2.0
                       if not hasattr(lines[name].geometry, "union_all")
                       else lines[name].geometry.union_all()   # Shapely 2.x
                 for name in lines}

    # -- 2. build a GeoSeries of the unique row/col polygons *just once*
    #       (boundary_cells repeats the same row/col for every layer)
    first_layer = {(r, c) for _, r, c in boundary_cells}        # set of tuples
    idx = [r * ncol + c for r, c in first_layer]                # flat indices
    uniq_polys = grid_polys.iloc[idx].geometry                  # GeoSeries

    # -- 3. vectorised distances → pick nearest bank for every unique poly
    dist_arr = np.vstack(
        [uniq_polys.distance(bank_geom[name]).values
         for name in ("left", "right", "up", "down")]
    ).T                                   # shape = (n_cells , 4)

    nearest_bank = np.argmin(dist_arr, axis=1)      # 0..3 → left/right/…
    bank_names   = np.array(["left", "right", "up", "down"])[nearest_bank]

    # -- 4. stash the mapping row/col → bank and fan it out to every layer
    mapping = {(r, c): bank
               for (r, c), bank in zip(first_layer, bank_names)}

    out: dict[str, list[tuple[int,int,int]]] = {k: [] for k in mapping.values()}
    for k, r, c in boundary_cells:
        out[mapping[(r, c)]].append((k, r, c))
    return out


# -------------------------------------------------------------------
#  Utility – infer rotation from the *centroids* of boundary cells
# -------------------------------------------------------------------
def rotation_from_boundary_cells(
    cells: list[tuple[int, int, int]],   # (k,row,col)
    grid_polys: gpd.GeoDataFrame,        # as built by build_grid_polygons()
    ncol: int,
) -> float:
    """
    Return the clockwise angle (deg) that makes the **X‑axis** of the model
    grid point from *left* → *right* banks.

    Algorithm
    ---------
    1.  Keep layer‑0 only (we just need the footprint).
    2.  Pull the centroids of those polygons.
    3.  Fit a simple linear regression  y = a·x + b .
        • slope *a*  →  atan(a)  (positive CCW)
    4.  Return **–atan(a)**  because Flopy’s `angrot` is **+CCW**.
    """
    import numpy as np

    # ── 1. unique (row,col) for layer‑0
    rc = {(r, c) for k, r, c in cells if k == 0}
    if len(rc) < 2:           # not enough information
        return 0.0

    # ── 2. centroids in global CRS
    centroids = [
        grid_polys.iloc[r * ncol + c].geometry.centroid.coords[0]
        for r, c in rc
    ]
    xs, ys = map(np.array, zip(*centroids))

    # ── 3. least‑squares line fit
    A = np.vstack([xs, np.ones_like(xs)]).T
    slope, _ = np.linalg.lstsq(A, ys, rcond=None)[0]

    # ── 4. angle (deg), clockwise
    ang_ccw = np.degrees(np.arctan(slope))
    return -ang_ccw            # Flopy: +CCW ⇒ negate for clockwise



def layer_for_elevation(tops:  list[np.ndarray],
                        botms: list[np.ndarray],
                        i: int, j: int,
                        stage: float) -> int | None:
    """
    Return the layer index *k* whose vertical span contains *stage*.

    ─ If *stage* > top of layer 0     → return 0  
    ─ If *stage* < bottom of last layer → **None** (outside model)  
    ─ Otherwise the first *k* with ``botm[k] ≤ stage ≤ top[k]``.  
    """
    if stage > tops[0][i, j]:
        return 0
    if stage < botms[-1][i, j]:
        return None

    for k, (top, botm) in enumerate(zip(tops, botms)):
        if botm[i, j] <= stage <= top[i, j]:
            return k
    return None                           # fallback – shouldn’t hit


# ──────────────────────────────────────────────────────────────────────
# Boundary-head interpolation
# ──────────────────────────────────────────────────────────────────────
from typing import List, Tuple


# ──────────────────────────────────────────────────────────────────────
# Ground-water elevation & CHD helpers
# ──────────────────────────────────────────────────────────────────────

def build_chd_data(
    river_cells      : list[tuple[int,int,int,float]],
    groups           : dict[str, list[tuple[int,int,int]]],
    heads            : dict[str, list[float]],
) -> list[list]:
    """
    Merge river-stage cells with constant-head boundary cells.
    River cells always win if a duplicate exists.
    """
    chd, seen = [], set()

    # 1) river-stage
    for k,i,j,h in river_cells:
        chd.append([k,i,j,float(h)])
        seen.add((k,i,j))

    # 2) other boundaries
    for side, cells in groups.items():
        for idx, (k,i,j) in enumerate(cells):
            if (k,i,j) in seen:
                continue                                   # skip duplicates
            chd.append([k,i,j,float(heads[side][idx])])
            seen.add((k,i,j))
    return chd


# ──────────────────────────────────────────────────────────────────────
# MODFLOW-6 groundwater-flow model builder
# ──────────────────────────────────────────────────────────────────────
import flopy
from numpy import full                           # local import keeps public API tidy
from typing import Tuple, Sequence, Any

def build_gwf_model(
    cfg: Any,                              # your pydantic Settings instance
    chd_data: Sequence[Sequence[float]],   # already-prepared CHD rows
    idomain: np.ndarray,                   # (nlay, nrow, ncol) activity mask
) -> Tuple[flopy.mf6.MFSimulation, flopy.mf6.ModflowGwf]:
    """
    Construct a *stand-alone* MODFLOW 6 GWF model using information held
    in ``cfg`` plus the caller-supplied ``chd_data`` and ``idomain`` array.

    Returns
    -------
    (sim, gwf) tuple – ready for ``sim.write_simulation()`` and
    ``sim.run_simulation()``.
    """
    # ── sanity checks -------------------------------------------------
    if cfg.nlay is None or cfg.nrow is None or cfg.ncol is None:
        raise ValueError("cfg.nlay / nrow / ncol must be populated first.")
    if idomain.shape != (cfg.nlay, cfg.nrow, cfg.ncol):
        raise ValueError("`idomain` dimensions don’t match cfg grid.")

    # ── 1.  Simulation container -------------------------------------
    sim = flopy.mf6.MFSimulation(
        sim_name   = cfg.sim_name,
        exe_name   = str(cfg.md6_exe_path),
        sim_ws     = str(cfg.gwf_ws),
    )

    flopy.mf6.ModflowTdis(
        sim,
        time_units = cfg.time_units.upper(),
        nper       = cfg.nper,
        perioddata = [(cfg.perlen, cfg.nstp, cfg.tsmult)],
    )

    # ── 2.  Groundwater-flow model -----------------------------------
    gwf = flopy.mf6.ModflowGwf(
        sim,
        modelname      = cfg.gwf_name,
        save_flows     = True,
    )

    flopy.mf6.ModflowGwfdis(
        gwf,
        nlay     = cfg.nlay,
        nrow     = cfg.nrow,
        ncol     = cfg.ncol,
        delr     = cfg.cell_size_x,
        delc     = cfg.cell_size_y,
        top      = cfg.tops[0],
        botm     = cfg.botm,
        idomain  = idomain,
        xorigin  = cfg.xmin,#cfg.xorigin,
        yorigin  = cfg.ymin,#cfg.yorigin,
        # angrot= cfg.grid_rotation_degrees,
        # crs=cfg.project_crs,
    )



    gwf.modelgrid.crs = cfg.project_crs
    # gwf.modelgrid.load_coord_info()
    gwf.modelgrid.set_coord_info(cfg.xmin, cfg.ymin, crs=cfg.project_crs)

    mg = gwf.modelgrid



    # ── 3.  Packages --------------------------------------------------
    # 3-a  initial heads – flat at bed elevation unless caller changed it
    strt = full((cfg.nlay, cfg.nrow, cfg.ncol), cfg.bed_elevation, dtype=float)
    flopy.mf6.ModflowGwfic(gwf, strt=strt)

    # 3-b  hydraulic properties
    flopy.mf6.ModflowGwfnpf(
        gwf,
        icelltype = 2,
        k         = cfg.kh,
        k33       = cfg.kv,
        save_flows        = True,
        save_saturation   = True,
        save_specific_discharge = True,
    )

    # 3-c  constant-head boundaries (river stage rows already embedded)
    if chd_data:
        flopy.mf6.ModflowGwfchd(
            gwf,
            maxbound           = len(chd_data),
            stress_period_data = {0: chd_data},
            save_flows         = True,
        )

    # 3-d  output control
    flopy.mf6.ModflowGwfoc(
        gwf,
        saverecord       = [("HEAD", "ALL"), ("BUDGET", "ALL")],
        head_filerecord  = [cfg.headfile],
        budget_filerecord= [cfg.budgetfile],
        printrecord      = [("HEAD", "LAST")],
    )

    # ── 4.  IMS solver ------------------------------------------------
    flopy.mf6.ModflowIms(
        sim,
        print_option      = "SUMMARY",
        outer_dvclose     = 1e-4,
        outer_maximum     = 200,
        inner_maximum     = 500,
        inner_dvclose     = 1e-4,
        rcloserecord      = 1e-4,
        linear_acceleration = "BICGSTAB",
        relaxation_factor = 0.97,
    )


    # ── 2‑b.  Tell the big arrays to live in external *binary* files ──────
    # put them under  <model_ws>/arrays/   so they are easy to find / clean

    external_dir = Path(gwf.model_ws) / "arrays"
    external_dir.mkdir(exist_ok=True)

    dis = gwf.get_package("DIS")
    ic  = gwf.get_package("IC")

    def _layered_records(array: np.ndarray, basename: str) -> list[dict]:
        return [{
            "filename": str(PurePath("arrays") / f"{basename}_L{lay+1}.bin"),
            "binary": True,
            "data": np.asarray(array[lay]),  # <-- convert to regular ndarray
            "iprn": 0,
            "factor": 1.0,
        } for lay in range(array.shape[0])]
    
    # Externalize DIS arrays
    if dis is not None:
        dis.top.set_record({
            "filename": str(PurePath("arrays") / "top.bin"),
            "binary": True,
            "data": np.asarray(dis.top.array),  # <-- fix here
            "iprn": 0,
            "factor": 1.0,
        })
        dis.botm.set_record(_layered_records(dis.botm.array, "botm"))
        if hasattr(dis, "idomain") and dis.idomain.array is not None:
            dis.idomain.set_record(_layered_records(dis.idomain.array, "idomain"))

    # Externalize IC array (initial conditions)
    if ic is not None:
        ic.strt.set_record(_layered_records(ic.strt.array, "strt"))

    # -------------------------------------------------------------------------
    # (optional) speed tweak – disable expensive size & data verification
    # -------------------------------------------------------------------------
    # sim = gwf.simulation
    # sim.simulation_data.auto_set_sizes = False
    # sim.simulation_data.verify_data    = False
    # or:  sim.simulation_data.lazy_io = True      # equivalent one‑liner



    return sim, gwf

# ---------------------------------------------------------------------
# MODPATH 7 helpers
# ---------------------------------------------------------------------
from pathlib import Path
import flopy
from flopy.modpath import Modpath7, ParticleData

def build_particle_models(
    sim_name: str,
    gwf: flopy.mf6.ModflowGwf,
    river_cells: list[tuple[int, int, int]],
    *,
    mp7_ws: Path | str | None = None,
    exe_path: str | Path | None = None,
) -> tuple[Modpath7, Modpath7]:
    """
    Create **forward** *and* **backward** MODPATH 7 models that start particles
    in every cell listed in *river_cells*.

    Parameters
    ----------
    sim_name      : str
        Base name – “_mp_forward / _mp_backward” are appended automatically.
    gwf           : flopy.mf6.ModflowGwf
        The built groundwater-flow model whose grid MODPATH inherits.
    river_cells   : list[(k, i, j)]
        Sequence of (layer,row,col) indices where particles should be released.
    mp7_ws, exe_path
        Optional overrides.  If omitted workspace & exe are taken from *gwf*
        (``gwf.simulation.sim_path`` and ``os.getenv("MP7")``).

    Returns
    -------
    (mp_forward, mp_backward) : tuple[flopy.modpath.Modpath7, Modpath7]
    """
    # ── defaults from the GWF simulation ──────────────────────────────
    if mp7_ws is None:
        mp7_ws = Path(gwf.simulation.sim_path).parent / "mp7_workspace"
    mp7_ws = Path(mp7_ws).absolute()
    mp7_ws.mkdir(exist_ok=True)

    if exe_path is None:
        exe_path = "mp7"   # rely on PATH

    def _make(direction: str) -> flopy.modpath.Modpath7:
        mp = Modpath7.create_mp7(
            modelname=f"{sim_name}_mp_{direction}",
            trackdir=direction,
            flowmodel=gwf,
            model_ws=mp7_ws,
            exe_name=str(exe_path),
            rowcelldivisions=1,
            columncelldivisions=1,
            layercelldivisions=1,
        )
        
        #print("Packages in MODPATH model:", mp.packagelist)

        # keep only (k, i, j) – ParticleData needs exactly three ints per tuple
        partlocs = [(k + 4, i, j) for (k, i, j, *_) in river_cells]

        # first positional arg = locations, keyword only for 'structured'
        _particledata = flopy.modpath.ParticleData(partlocs, structured=True, drape=0)
        pg = flopy.modpath.ParticleGroup(particledata=_particledata)

        mpsim : flopy.modpath.Modpath7Sim = mp.get_package("MPSIM")          # <‑‑ this replaces mpsim_data
        mpsim.particlegroups.clear()          # remove any existing groups
        mpsim.particlegroups.append(pg)          # add the particle group

        # mpbas = flopy.modpath.Modpath7Bas(mp, porosity=0.1)

        return mp

    mp_forward  = _make("forward")
    mp_backward = _make("backward")
    return mp_forward, mp_backward

# ---------------------------------------------------------------------
#  I/O + execution helpers
# ---------------------------------------------------------------------
def write_models(*sims, silent: bool = False) -> None:
    """
    Write all input files for the passed simulations / models.

    Accepts any combination of
    
    • flopy.mf6.MFSimulation – has ``write_simulation``  
    • flopy.modpath.Modpath7 – has ``write_input``

    Extra positional arguments are ignored, so you can do

        write_models(gwf_sim, mp_fwd, mp_back, silent=True)
    """
    for sim in sims:
        if hasattr(sim, "write_simulation"):        # MF6 simulation
            sim.write_simulation(silent=silent)
        elif hasattr(sim, "write_input"):           # MP7 model
            sim.write_input()
        else:
            raise TypeError(f"Don’t know how to write files for {type(sim)}")


def run_models(*sims, silent: bool = False) -> None:
    """
    Run the passed simulations / models in sequence, aborting on the first
    failure.  Works with both MF6 (``run_simulation``) and MP7
    (``run_model``) back-ends.
    """
    for sim in sims:
        if hasattr(sim, "run_simulation"):          # MF6 simulation
            ok, _ = sim.run_simulation(silent=silent, report=True)
        elif hasattr(sim, "run_model"):             # MP7 model
            ok, _ = sim.run_model(silent=silent, report=True)
        else:
            raise TypeError(f"Don’t know how to run {type(sim)}")

        if not ok:
            raise RuntimeError(f"❌  Model run failed → {sim.name}")
        print(f"✅  Model run finished → {sim.name}")



# ────────────────────────────────────────────────────────────────────
# YAML GUI editor  – Tkinter modal dialog
# ────────────────────────────────────────────────────────────────────
"""YAML inputs editor – comment‑preserving, sectioned, file‑picker and tooltips.

Features
========
• **Sections**: grouped by the comment header preceding each block.
• **Vertical scroll** inside the window (mouse‑wheel friendly).
• **Browse…** button for any field whose inline comment contains the word
  *filepath* (case‑insensitive).  If the comment also contains an extension in
  square brackets, e.g. `# terrain filepath [tif]`, the dialog filters to that
  extension.
• **Hover tooltips**: if the text inside an entry is wider than the widget,
  hovering shows the full value in a yellow tooltip.
• **Comment & header preservation** on save.  Only the scalar part of each
  `key: value  # comment` line is replaced.
• **Save As…** creates parent folders automatically.

Limitations
-----------
Flat `key: scalar` structures only – lists / dicts aren’t rewritten.
"""

import tkinter as tk
from tkinter import ttk, filedialog, messagebox
from pathlib import Path
from collections import OrderedDict
import yaml, re
from typing import Any, Dict, List, Tuple

# ────────────────────────────────────────────────────────────────────
# YAML helpers
# ────────────────────────────────────────────────────────────────────

def _parse_yaml(path: Path) -> Tuple[Dict[str, Any], OrderedDict[str, List[str]], Dict[str, str], Dict[str, int]]:
    """Return (data, sections, descriptions, line_map)."""
    lines = path.read_text(encoding="utf-8").splitlines()
    sanitized = [ln for ln in lines if not re.fullmatch(r"\s*\.\.\.\s*", ln)]
    try:
        data: Dict[str, Any] = yaml.safe_load("\n".join(sanitized)) or {}
    except yaml.YAMLError:
        data = {}

    sections: OrderedDict[str, List[str]] = OrderedDict()
    descs: Dict[str, str] = {}
    line_map: Dict[str, int] = {}

    current, divider, pending = "General", False, []
    for idx, line in enumerate(lines):
        s = line.lstrip()
        if re.fullmatch(r"\s*\.\.\.\s*", s):
            continue
        if s.startswith("#"):
            txt = s.lstrip("#").strip()
            if re.fullmatch(r"-+", txt):
                divider = True
                continue
            if divider and txt:
                current = txt
                sections.setdefault(current, [])
                divider = False
                continue
            pending.append(txt)
            continue
        if ":" in line:
            key, rest = line.split(":", 1)
            key = key.strip()
            inline = rest.split("#", 1)[1].strip() if "#" in rest else ""
            descs[key] = "\n".join(filter(None, [*pending, inline])).strip()
            sections.setdefault(current, []).append(key)
            line_map[key] = idx
            pending.clear()
            continue
        pending.clear()
    return data, sections, descs, line_map


def _scalar(val: Any) -> str:
    if isinstance(val, bool):
        return "true" if val else "false"
    if isinstance(val, (int, float)):
        return str(val)
    return val if re.fullmatch(r"[\w./-]+", str(val)) else yaml.safe_dump(val).strip()

# ────────────────────────────────────────────────────────────────────
# Tooltip helper
# ────────────────────────────────────────────────────────────────────
class _Tooltip:
    def __init__(self, widget: tk.Widget, text_fn):
        self.widget, self.text_fn, self.tip = widget, text_fn, None
        widget.bind("<Enter>", self._show)
        widget.bind("<Leave>", self._hide)

    def _show(self, _):
        txt = self.text_fn()
        if not txt or self.widget.winfo_reqwidth() < self.widget.winfo_width():
            return
        x = self.widget.winfo_rootx() + 15
        y = self.widget.winfo_rooty() + self.widget.winfo_height() + 2
        self.tip = tk.Toplevel(self.widget)
        self.tip.wm_overrideredirect(True)
        self.tip.geometry(f"+{x}+{y}")
        tk.Label(self.tip, text=txt, background="#FFFFE0", relief="solid", borderwidth=1, justify="left").pack()

    def _hide(self, _):
        if self.tip:
            self.tip.destroy(); self.tip = None

# ────────────────────────────────────────────────────────────────────
# Main editor
# ────────────────────────────────────────────────────────────────────
def launch_inputs_editor(yaml_path: str | Path = "inputs.yaml") -> None:  # noqa: C901
    yaml_path = Path(yaml_path).expanduser()
    data, sections, descs, line_map = _parse_yaml(yaml_path)
    orig_lines = yaml_path.read_text(encoding="utf-8").splitlines()

    root = tk.Toplevel()
    root.title(f"inputs.yaml — {yaml_path}")
    root.geometry("900x650")
    root.grab_set()
    ttk.Style(root).configure("Section.TLabelframe.Label", font=("Segoe UI", 11, "bold"))

    changed = tk.BooleanVar(value=False)
    path_var = tk.StringVar(value=str(yaml_path))
    widgets: Dict[str, Tuple[tk.Entry, tk.StringVar]] = {}

    # ---------- file helpers ----------
    def _write_yaml():
        p = Path(path_var.get()); p.parent.mkdir(parents=True, exist_ok=True)
        lines = p.read_text().splitlines() if p.exists() else orig_lines.copy()
        for key, (_, var) in widgets.items():
            scalar = _scalar(var.get().strip())
            if key not in line_map or line_map[key] >= len(lines):
                lines.append(f"{key}: {scalar}"); continue
            idx = line_map[key]
            pre, rest = lines[idx].split(":", 1)
            before, *cmt = rest.split("#", 1)
            indent = " " if before.startswith(" ") else ""
            new = f"{indent}{scalar}" + (f"  # {cmt[0].strip()}" if cmt else "")
            lines[idx] = f"{pre}:{new}"
        p.write_text("\n".join(lines) + "\n", encoding="utf-8"); changed.set(False)

    def _save(as_new=False):
        if as_new:
            fname = filedialog.asksaveasfilename(parent=root, defaultextension=".yaml", filetypes=[("YAML", "*.yaml *.yml"), ("All", "*.*")])
            if not fname:
                return
            path_var.set(fname)
        try:
            _write_yaml(); messagebox.showinfo("Saved", f"File written to\n{path_var.get()}")
        except Exception as exc:
            messagebox.showerror("Save error", str(exc))

    def _open():
        if changed.get() and not messagebox.askyesno("Unsaved changes", "Discard unsaved changes?"):
            return
        fname = filedialog.askopenfilename(parent=root, filetypes=[("YAML", "*.yaml *.yml")])
        if not fname:
            return
        nonlocal data, sections, descs, line_map, orig_lines
        data, sections, descs, line_map = _parse_yaml(Path(fname))
        orig_lines = Path(fname).read_text().splitlines(); path_var.set(fname)
        _build(); changed.set(False)

    # ---------- UI scaffold ----------
    tb = ttk.Frame(root); tb.pack(fill="x", padx=4, pady=4)
    ttk.Button(tb, text="Open…", command=_open).pack(side="left", padx=2)
    ttk.Button(tb, text="Save", command=_save).pack(side="left", padx=2)
    ttk.Button(tb, text="Save As…", command=lambda: _save(True)).pack(side="left", padx=2)

    canvas = tk.Canvas(root, highlightthickness=0)
    vbar = ttk.Scrollbar(root, orient="vertical", command=canvas.yview)
    canvas.configure(yscrollcommand=vbar.set)
    vbar.pack(side="right", fill="y"); canvas.pack(side="left", fill="both", expand=True)
    inner = ttk.Frame(canvas); canvas.create_window((0, 0), window=inner, anchor="nw")
    inner.bind("<Configure>", lambda _e: canvas.configure(scrollregion=canvas.bbox("all")))
    for ev in ("<MouseWheel>", "<Button-4>", "<Button-5>"):
        canvas.bind_all(ev, lambda e: canvas.yview_scroll(int(-e.delta/120) if hasattr(e, 'delta') else (1 if e.num==5 else -1), "units"))

    # ---------- builders ----------
    def _browse(var: tk.StringVar, desc: str):
        m = re.search(r"\[([A-Za-z0-9]+)\]", desc)
        filt = [(f"*.{m.group(1)}", f"*.{m.group(1)}")] if m else [("All", "*.*")]
        f = filedialog.askopenfilename(parent=root, filetypes=filt)
        if f:
            var.set(f); changed.set(True)

    def _build():
        for w in inner.winfo_children():
            w.destroy()
        widgets.clear()
        row_frame = 0
        for section, keys in sections.items():
            lf = ttk.Labelframe(inner, text=section, style="Section.TLabelframe", padding=(6,4))
            lf.grid(row=row_frame, column=0, sticky="ew", padx=6, pady=6)
            lf.columnconfigure(1, weight=1)
            for r, key in enumerate(keys):
                desc = descs.get(key, "")
                ttk.Label(lf, text=key).grid(row=r, column=0, sticky="w", padx=4, pady=2)
                var = tk.StringVar(value=str(data.get(key, "")))
                # Entry + optional browse
                if "filepath" in desc.lower():
                    rowf = ttk.Frame(lf); rowf.grid(row=r, column=1, sticky="ew", padx=4, pady=2); rowf.columnconfigure(0, weight=1)
                    ent = ttk.Entry(rowf, textvariable=var)
                    ent.grid(row=0, column=0, sticky="ew")
                    ttk.Button(rowf, text="Browse…", command=lambda v=var, d=desc: _browse(v, d)).grid(row=0, column=1, padx=3)
                else:
                    ent = ttk.Entry(lf, textvariable=var, width=40); ent.grid(row=r, column=1, sticky="ew", padx=4, pady=2)
                _Tooltip(ent, var.get)
                ttk.Label(lf, text=desc, wraplength=320, justify="left").grid(row=r, column=2, sticky="w", padx=4, pady=2)
                var.trace_add("write", lambda *_: changed.set(True))
                widgets[key] = (ent, var)
            row_frame += 1
        inner.update_idletasks()
        root.geometry(f"{inner.winfo_reqwidth() + vbar.winfo_width() + 20}x{root.winfo_height()}")

    _build()

    def _on_close():
        if changed.get() and messagebox.askyesno("Unsaved changes", "Save before exit?"):
            _save(False)
        root.destroy()

    root.protocol("WM_DELETE_WINDOW", _on_close)
    root.wait_window(root)

# ────────────────────────────────────────────────────────────────────
#  High-level helper ─ build *all* grid-related arrays in one shot
# ────────────────────────────────────────────────────────────────────
def build_full_grid(cfg) -> None:
    """
    Build every grid-related array exactly as in your original notebook,
    then stash them back on *cfg* for later reuse (GUI, __main__.py…).

    This version uses **pv.ImageData** (alias of the old UniformGrid).
    """
    from functions import raster_utils as ru

    # --- 1. Terrain raster --------------------------------------------------
    arr, tfm, crs, nodata, bounds_box = ru.load_raster(cfg.terrain_output_raster)
    # terrain = ru.mask_nodata(arr, nodata)
    terrain = ru.mask_nodata(arr, nodata).filled(np.nan)      # <<< add .filled

    import matplotlib.pyplot as plt

    # # Display the terrain raster in a popup window
    # plt.figure(figsize=(10, 8))
    # plt.imshow(terrain, cmap="terrain", origin="upper")
    # plt.colorbar(label="Elevation")
    # plt.title("Terrain Output Raster")
    # plt.xlabel("Column Index")
    # plt.ylabel("Row Index")
    # plt.show()

    xmin, ymin, xmax, ymax = ru.raster_extent(tfm, arr.shape[1], arr.shape[0])
    width_ft  = xmax - xmin
    height_ft = ymax - ymin

    ncol, nrow = ru.grid_dimensions(width_ft, height_ft,
                                    cfg.cell_size_x, cfg.cell_size_y)

    grid_x, grid_y = ru.generate_grid_centres(ncol, nrow,
                                              cfg.cell_size_x, cfg.cell_size_y,
                                              xmin, ymin, origin="upper")
    grid_points = ru.grid_to_geodataframe(grid_x, grid_y, crs)

    # # Plot the grid points and terrain raster on the same plot
    # plt.figure(figsize=(10, 8))
    # plt.imshow(terrain, cmap="terrain", origin="upper", extent=[xmin, xmax, ymin, ymax], alpha=0.7)
    # plt.colorbar(label="Elevation")
    # plt.scatter(grid_x, grid_y, s=1, c="blue", label="Grid Points")
    # plt.title("Grid Points and Terrain Raster")
    # plt.xlabel("X Coordinate")
    # plt.ylabel("Y Coordinate")
    # plt.legend()
    # plt.show()

    # --- 2. Top layer (sample & fill) --------------------------------------
    top = np.full((nrow, ncol), np.nan)
    for i in range(nrow):
        for j in range(ncol):
            col, row = ~tfm * (grid_x[i, j], grid_y[i, j])
            col, row = int(col), int(row)
            if 0 <= row < terrain.shape[0] and 0 <= col < terrain.shape[1]:
                top[i, j] = terrain[row, col]
    top = ru.interpolate_na(np.ma.masked_invalid(top)).filled(np.nanmedian(top))


    # # ── NEW QA PLOT – “top” layer in real‑world coords ----------------------
    # plt.figure(figsize=(10, 8))
    # plt.imshow(
    #     top,
    #     cmap="terrain",
    #     origin="lower",                 # model layer orientation
    #     extent=[xmin, xmax, ymin, ymax],
    #     interpolation="nearest",
    # )
    # plt.colorbar(label="Top‑layer elevation (ft)")
    # plt.title("Model top layer (cell‑center elevations)")
    # plt.xlabel("Easting")
    # plt.ylabel("Northing")
    # plt.gca().set_aspect("equal")
    # plt.show()


    # --- 3. Layer stack -----------------------------------------------------
    required = {
        "cell_size_x": cfg.cell_size_x,
        "cell_size_y": cfg.cell_size_y,
        "z":           cfg.z,
    }
    missing = [k for k, v in required.items() if v in (None, "", "null")]
    if missing:
        raise ValueError(
            "Missing required parameter(s) in inputs.yaml: "
            + ", ".join(missing)
        )

    # layer count: prefer an explicit nlay, otherwise gw_mod_depth / z,
    # otherwise default to 40 layers
    try:
        if getattr(cfg, "nlay", None) not in (None, "", "null"):
            nlay_target = int(float(cfg.nlay))
        elif getattr(cfg, "gw_mod_depth", None) not in (None, "", "null"):
            nlay_target = int(np.ceil(float(cfg.gw_mod_depth) / float(cfg.z))) #+ 1
        else:
            nlay_target = 40
    except (TypeError, ValueError):
        raise ValueError(
            "nlay / gw_mod_depth / z must be numeric values in inputs.yaml."
        )

    bed_elev     = float(terrain.min())
    first_botm   = np.full_like(top, bed_elev)

    tops  = [top]
    botms = [first_botm]
    for _ in range(1, nlay_target):
        next_top  = botms[-1]
        next_botm = next_top - float(cfg.z)
        tops.append(next_top)
        botms.append(next_botm)

    

    # --- 4. Persist to cfg --------------------------------------------------
    cfg.raster_transform   = tfm
    cfg.raster_crs         = crs
    cfg.raster_bounds_box  = bounds_box
    cfg.terrain_elevation  = terrain
    cfg.bed_elevation      = bed_elev

    cfg.ncol, cfg.nrow     = ncol, nrow
    cfg.grid_x, cfg.grid_y = grid_x, grid_y
    cfg.grid_points        = grid_points
    gwd_union = _union_all(cfg.ground_water_domain)       # NEW
    cfg.intersecting_points = (
        grid_points[grid_points.geometry.intersects(gwd_union)]
        if getattr(cfg, "ground_water_domain", None) is not None
        else grid_points
    )

    cfg.xmin, cfg.ymin, cfg.xmax, cfg.ymax = xmin, ymin, xmax, ymax

    cfg.top   = top
    cfg.tops  = tops
    cfg.botm  = botms
    cfg.nlay  = len(tops)



    #cfg.grid_rotation_deg = float(cfg.grid_rotation_deg)  # ← keep YAML value
    # or, if you need to compute it:
    #   # angle of the left‑bank polyline


# ──────────────────────────────────────────────────────────────────────
#  High–level orchestration
#      – idomain + boundary discovery + CHD rows
# ──────────────────────────────────────────────────────────────────────
def prepare_idomain_and_chd(cfg):
    """
    One-stop helper that replicates all the geo-analysis you used in the
    notebook / __main__.py:

    1.  Build **idomain** – a (nlay, nrow, ncol) 0/1 mask telling MODFLOW
        which cells are active.
    2.  Identify boundary cells (left / right / up-stream / down-stream).
    3.  Sample surface-water raster ⇒ river-stage cells.
    4.  Interpolate groundwater heads along the banks.
    5.  Merge everything into a ready-to-use **CHD package** list.

    The function returns *(idomain, chd_data)* **and also stores them back on
    `cfg`** for later GUI calls.

    Notes
    -----
    • Assumes `build_full_grid(cfg)` has already run (so grid_x, grid_y,
      tops, botm, nlay, … all exist).
    • Re-uses the fast helpers already defined in this module.
    """
    import numpy as np

    # ═════════════════ 1) grid polygons + idomain ═════════════════════
    grid_polys = build_grid_polygons(
        cfg.grid_x, cfg.grid_y,
        cfg.cell_size_x, cfg.cell_size_y,
        cfg.raster_crs,
    )

    idomain = idomain_from_domain(
        grid_polys,
        cfg.ground_water_domain,          # GeoDataFrame read by setup_vectors()
        cfg.nlay, cfg.nrow, cfg.ncol,
    )

    # ═════════════════ 2) classify lateral boundaries ═════════════════
    up_bd, down_bd = make_up_down_stream(
        cfg.left_boundary,
        cfg.right_boundary,
        cfg.raster_crs,
    )

    b_cells   = identify_boundary_cells(idomain)
    b_groups  = classify_boundary_cells_fast(
        b_cells,
        grid_polys,
        {"left":  cfg.left_boundary,
         "right": cfg.right_boundary,
         "up":    up_bd,
         "down":  down_bd},
        cfg.ncol,
    )

    # # ── NEW – infer rotation and stash on cfg ---------------------------
    # cfg.grid_rotation_degrees = rotation_from_boundary_cells(
    #     b_groups["down"],     # ← left bank is the grid *row* direction
    #     grid_polys,
    #     cfg.ncol,
    # )
    # print(f"[INFO] Grid rotation = {cfg.grid_rotation_degrees:.3f}° clockwise")

    # # ═════════════════ 2.1) grid rotation (optional) ══════════════════
    # _dx = cfg.left_boundary.geometry.iloc[0].coords[-1][0] \
    #     - cfg.left_boundary.geometry.iloc[0].coords[0][0]
    # _dy = cfg.left_boundary.geometry.iloc[0].coords[-1][1] \
    #     - cfg.left_boundary.geometry.iloc[0].coords[0][1]
    # cfg.grid_rotation_degrees = -math.degrees(math.atan2(_dy, _dx))  # clockwise +

    # ═════════════════ 3) river-stage cells from cropped WSE raster ═══
    csv_df = csv_points_elevation(
        cfg.grid_points,
        cfg.cropped_water_surface_raster,
        out_csv=None,
    )
    csv_df = fit_csv_to_grid(
        csv_df,
        cfg.ncol, cfg.nrow,
        cfg.xmin, cfg.ymin, cfg.xmax, cfg.ymax,
    )

    river_cells = extract_river_cells(
        csv_df,
        idomain,
        cfg.tops,
        cfg.botm,
    )

    # ═════════════════ 4) heads along the four banks ══════════════════
    max_up   = csv_df.elevation.max()
    max_down = csv_df.elevation.min()
    offset   = cfg.gw_offset              # YAML field (distance below WSE)

    heads = {
        "left":  interpolate_gw_elevation(
                     [c for c in b_groups["left"] if c[0] == 0],
                     max_up   + offset,
                     max_down + offset,
                     cfg.nlay),
        "right": interpolate_gw_elevation(
                     [c for c in b_groups["right"] if c[0] == 0],
                     max_up   + offset,
                     max_down + offset,
                     cfg.nlay),
        "up":    [max_up   + offset] * len(b_groups["up"]),
        "down":  [max_down + offset] * len(b_groups["down"]),
    }

    # ═════════════════ 5) final CHD rows  ═════════════════════════════
    chd_data = build_chd_data(
        river_cells,
        b_groups,
        heads,
    )

    # stash for later reuse (viewer, reports, etc.)
    cfg.idomain  = idomain
    cfg.chd_data = chd_data

    return idomain, chd_data
